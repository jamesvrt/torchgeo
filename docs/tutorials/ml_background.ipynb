{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction for Deep learning practicioners new to remote sensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- explain that there are largely two types of datasets in torchgeo\n",
    "- explain vision datasets, explain geodatasets (most of them do not contain raw satellite imagery, need to pair)\n",
    "- highlight difference between them\n",
    "- what is special about remote sensing imagery as compared to imagenet\n",
    "- crs units explanation\n",
    "- projection that transforms the surface of a 3D earth onto a 2D planar image that we are used to\n",
    "- as mentioned, remote sensing datasets consist of large tiles, how to do tiling (process of splitting a larger tile into smaller patches or chips)\n",
    "- explain how this works in torchgeo\n",
    "- why the chips can be of non-constant size\n",
    "- how to return constant size chips that can be part of a dataloader\n",
    "- so how to construct a dataloader with geosampler, explain the arguments of the geoSampler\n",
    "- how to create datasplits if they are not predefined or not available\n",
    "- how to use torchgeo models with and without pytorch lightning modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this tutorial is to make users with a Deep Learning background that are new to remote sensing familiar with the TorchGeo library. In general, remote sensing datasets often contain much higher dimensional images than usual image datasets. Since, deep learning models sometimes require a given image input dimension or face memory issues for large input images, a main objective of this tutorial will be to demonstrate how to do this for all types of TorchGeo Datasets.\n",
    "\n",
    "It is recommended to run this notebook on Google Colab if you do not have your own GPU. Click the \"Open in Colab\" butto above to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install TorchGeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torchgeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import libraries that we require for this tutorial. And set a temporary data directory from which to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'stack_samples' from 'torchgeo.datasets' (/home/nils/.virtualenvs/torchGeoEnv/lib/python3.8/site-packages/torchgeo/datasets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22085/1173108927.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchgeo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNAIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChesapeakeDE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchgeo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloveda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoveDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchgeo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'stack_samples' from 'torchgeo.datasets' (/home/nils/.virtualenvs/torchGeoEnv/lib/python3.8/site-packages/torchgeo/datasets/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchgeo.datasets import NAIP, ChesapeakeDE, stack_samples\n",
    "from torchgeo.datasets.loveda import LoveDA\n",
    "from torchgeo.datasets.utils import download_url\n",
    "from torchgeo.samplers import RandomGeoSampler\n",
    "\n",
    "data_root = tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchGeo Datasets\n",
    "\n",
    "In TorchGeo, we differentiate between two types of datasets: Geospatial Datasets and Non-geospatial Datasets. While the former includes geospatial information that we will look at in more detail, the latter do not. List other differences... We will now take a closer look at Non-geospatial datasets before we turn our attention to Geospatial datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-geospatial Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For people familiar with torchvision datasets and their combination with Pytorch DataLoaders, TorchGeo's Non-geospatial datasets will work the same way except that by design, TorchGeo returns a dictionary containing a sample of the chosen dataset. For example, to use the [LoveDA Land-Cover Dataset](https://arxiv.org/abs/2110.08733), a semantic segmentation dataset with RGB imagery, you can simply do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n",
      "Downloading https://zenodo.org/record/5706578/files/Val.zip?download=1 to /tmp/loveda/Val.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2425958400it [05:39, 7144678.39it/s]                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/loveda/Val.zip to /tmp/loveda\n"
     ]
    }
   ],
   "source": [
    "loveda_root = os.path.join(data_root, \"loveda\")\n",
    "ds = LoveDA(root=loveda_root, split=\"val\", download=True)\n",
    "dl = DataLoader(ds, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for sample in dl:\n",
    "    image = sample[\"image\"]\n",
    "    mask = sample[\"mask\"]\n",
    "\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we use the smaller validation split because it downloads faster and it is just for demonstration purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can observe that the default image dimensions of this dataset are 1024x1024. If we would like to change this, we can pass a custom transformation - for example random cropping - when instantiating a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom image dataset size transform\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop the image and label to a given size\"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        \"\"\"Initialize a Random Crop Object for torchgeo samples\n",
    "\n",
    "        Args:\n",
    "            output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "        \"\"\"\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"Random Crop the image and label\n",
    "\n",
    "        Args:\n",
    "            sample (dict): sample returned from torchgeo dataset\n",
    "        \"\"\"\n",
    "        img, mask = sample[\"image\"], sample[\"mask\"].squeeze(0)\n",
    "        h, w = img.shape[1:]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = torch.randint(low=0, high=(h - new_h), size=(1,)).item()\n",
    "        left = torch.randint(low=0, high=(w - new_w), size=(1,)).item()\n",
    "\n",
    "        img = img[:, top : top + new_h, left : left + new_w]\n",
    "        mask = mask[top : top + new_h, left : left + new_w]\n",
    "\n",
    "        return {\"image\": img, \"mask\": mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use this transform by passing it as an argument when instantiating the dataset. You can then see that the image and mask have the desired dimension of 256x256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n",
      "torch.Size([5, 3, 256, 256])\n",
      "torch.Size([5, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# use the transform and demonstrate\n",
    "ds = LoveDA(root=loveda_root, split=\"val\", transforms=RandomCrop(256))\n",
    "dl = DataLoader(ds, batch_size=8)\n",
    "\n",
    "# just return one sample here to demonstrate not ful iteration\n",
    "for sample in dl:\n",
    "    image = sample[\"image\"]\n",
    "    mask = sample[\"mask\"]\n",
    "\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As, we can see, samples from the dataloader now have the desired dimensions. Hopefully, you now have a good understanding of how TorchGeo's Non-geospatial datasets work, how easy they are to use, and how you can adapt a dataset to your liking through custom transforms. We will now turn our attention to Geospatial datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geospatial Datasets, implemented as GeoDataset class in TorchGeo, contain geospatial information such as latitude, longitude, coordinate stystem, and projection. Furthermore, Geospatial Datasets can contain Raster or Vector Data (Explain difference here). When working with remote sensory data such as satellite imagery, there occurs a projection that transforms the surface of the 3D Globe onto a 2D image that we are used to seeing. Additionally, the data should include the precise geographical location of a captured image. For this purpose cartographers have developed a framework called the coordinate reference system (CRS), which aims to display the spherical earh in just two dimensions. Due to this, all projections will yield some distortion of distance, area, or angular properties! A nice example of, how different CRS distort a given map, can be found here. There exist many different CRS (link here) that each have their advantages and disadvantages depending on the location, scale, and purpose of a map. As a consequence, TorchGeo's Geospatial datasets have a crs parameter, that lets you project the given dataset as you deem useful. \n",
    "\n",
    "Another substantial difference between TorchGeo's Geospatial Datasets and common Pytorch datasets is that the GeoDataset does not index the dataset with an integer, but rather with a Bounding Box consisting of *geographical* coordinates. More specifically, every GeoDataset has a dataset.index object which is an R-tree (link here) containing the Bounding Boxes and filenames of all raster or vector files in the dataset. In TorchGeo there exists a custom GeoSampler that takes care of the indexing into geospatial datsets.\n",
    "\n",
    "Before we finally take a closer look at how to set up the data pipeline, it should be mentioned that a majority of TorchGeo's geospatial datasets do not contain both images and labels, but only one of the two. This means that if we are interested in a certain TorchGeo dataset that only contains labels and want to train a model to make predictions from raw satellite imagery, we need to pair this particular dataset with an available imagery dataset of the same resolution. In many cases, TorchGeo can do this automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Let us consider the data from [National Agriculture Imagery Program (NAIP)](https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/) and more specifically, just one state of it, Delaware. This dataset only contains labels, and hence, we need to pair it with satellite imagery from another source, for example [National Agriculture Imagery Program (NAIP)](https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/). Describe the process of choosing NAIP tiles here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://naipblobs.blob.core.windows.net/naip/v002/de/2018/de_060cm_2018/38075/m_3807511_ne_18_060_20181104.tif to /tmp/naip/m_3807511_ne_18_060_20181104.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "581770240it [02:10, 4461486.57it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://naipblobs.blob.core.windows.net/naip/v002/de/2018/de_060cm_2018/38075/m_3807511_se_18_060_20181104.tif to /tmp/naip/m_3807511_se_18_060_20181104.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "590502912it [02:36, 3761604.56it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://naipblobs.blob.core.windows.net/naip/v002/de/2018/de_060cm_2018/38075/m_3807512_nw_18_060_20180815.tif to /tmp/naip/m_3807512_nw_18_060_20180815.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "574469120it [02:28, 3879767.84it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://naipblobs.blob.core.windows.net/naip/v002/de/2018/de_060cm_2018/38075/m_3807512_sw_18_060_20180815.tif to /tmp/naip/m_3807512_sw_18_060_20180815.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "569662464it [02:23, 3972849.49it/s]                               \n"
     ]
    }
   ],
   "source": [
    "naip_root = os.path.join(data_root, \"naip\")\n",
    "naip_url = \"https://naipblobs.blob.core.windows.net/naip/v002/de/2018/de_060cm_2018/38075/\"\n",
    "tiles = [\n",
    "    \"m_3807511_ne_18_060_20181104.tif\",\n",
    "    \"m_3807511_se_18_060_20181104.tif\",\n",
    "    \"m_3807512_nw_18_060_20180815.tif\",\n",
    "    \"m_3807512_sw_18_060_20180815.tif\",\n",
    "]\n",
    "for tile in tiles:\n",
    "    download_url(naip_url + tile, naip_root)\n",
    "\n",
    "naip = NAIP(naip_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can tell TorchGeo to automatically download corresponding Chesapeake labels, by passing in the crs and resolution argument from the NAIP dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://cicwebresources.blob.core.windows.net/chesapeakebaylandcover/DE/_DE_STATEWIDE.zip to /tmp/chesapeake/_DE_STATEWIDE.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287350784it [01:00, 4781939.91it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/chesapeake/_DE_STATEWIDE.zip to /tmp/chesapeake\n"
     ]
    }
   ],
   "source": [
    "chesapeake_root = os.path.join(data_root, \"chesapeake\")\n",
    "\n",
    "chesapeake = ChesapeakeDE(chesapeake_root, crs=naip.crs, res=naip.res, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature of TorchGeo are Union and IntersectionDataset, which allow us to combine two or more datasets into one, so that we can retrieve samples from it simultaneously. In this case we desire the intersection of our NAIP and Chesapeake dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'NAIP' and 'ChesapeakeDE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22085/3080924347.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaip\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mchesapeake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'NAIP' and 'ChesapeakeDE'"
     ]
    }
   ],
   "source": [
    "dataset = naip & chesapeake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike typical PyTorch Datasets, TorchGeo GeoDatsets are indexed using a bounding box consisting of latitude, longitude and time information. For this purpose TorchGeo provides a custom GeoSampler instead of the default sampler or batch_sampler from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomGeoSampler(naip, size=1000, length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size argument for the GeoSampler is in crs units, *not* pixel units, and defines how large of an area the sampled bounding box that the dataset gets indexed with should be. The length parameter decides how many samples this GeoSampler should return in total. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, sampler=sampler, collate_fn=stack_samples, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the dataloader available, now we can proceed with our training pipeline in a familiar fashion. Where the dataloader will return batches of size 2 and a total of 10 samples as dictated by the length argument to the GeoSampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataloader:\n",
    "    image = sample[\"image\"]\n",
    "    mask = sample[\"mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we desire a specific image dimension for Geospatial datasets, we can adapt the size parameter in our GeoSampler. As previously mentioned, the size argument is in crs units and *not* pixel units. We can work around it by recognizing that , and implementing that accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_size_pix = 256\n",
    "crs_size = desired_size_pix * dataset.res\n",
    "sampler = RandomGeoSampler(naip, size=crs_size, length=10)\n",
    "\n",
    "# just return one sample here for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloader = DataLoader(dataset, sampler=sampler, collate_fn=stack_samples)\n",
    "\n",
    "for sample in dataloader:\n",
    "    image = sample[\"image\"]\n",
    "    mask = sample[\"mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now observe that the image and mask have the desired pixel resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you have a better understanding of how to work with both Non-geospatial and geospatial datasets from TorchGeo. This tutorial just offered a small insight into the many different features TorchGeo offers."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "637d781892d0dd0c42fdeef27119474a252f7bd002a4c964d9b8fa78828250b9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('torchGeoEnv': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
